{"cells":[{"cell_type":"markdown","metadata":{"id":"XIyP_0r6zuVc"},"source":["# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n","\n","QLora Training on Social Media Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuXIFTFapAMI","outputId":"1688a296-5edd-4cd9-8578-59bca66040a3","scrolled":true},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets"]},{"cell_type":"markdown","metadata":{"id":"MJ-5idQwzvg-"},"source":["load the model - GPT-neo-x-20B (Note that the model itself is around 40GB in half precision)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["b3e71ade6ff440049922593cde6496e3"]},"id":"E0Nl5mWL0k2T","outputId":"6f29f5ae-8c14-470c-d3ac-b0ece6e0dfa6"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","\n","model_id = \"EleutherAI/gpt-neox-20b\"\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token = tokenizer.eos_token\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"]},{"cell_type":"markdown","metadata":{"id":"Mp2gMi1ZzGET"},"source":["Apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9EUEDAl0ss3"},"outputs":[],"source":["model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkIcwsSU01EB"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ybeyl20n3dYH","outputId":"1414da36-abed-496f-d369-e0a85605b186"},"outputs":[],"source":["config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"query_key_value\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{"id":"FCc64bfnmd3j"},"source":["Load Dataset and Map"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["f002685d984647c3a381e0e333098cab","b69429c12e024b3fbac606e73de84c00","ce74cd26fc8c4caa8c659ca4f6d6dede","f634ed71900e44afb5a9e34803e04ec5","9c300080f3e4441bbf45bfa2cb7a2e35","7f52e58d73ae4a688164274ff71b9889","2b0d9edbed9f4ceca4022cad146ce4ad","90be808ceaf74a1ab01396e0a707a207"]},"id":"s6f4z8EYmcJ6","outputId":"213e2298-ce91-45b9-cdeb-9084f18c55ad"},"outputs":[],"source":["from datasets import load_dataset\n","import datasets\n","\n","dataset1 = load_dataset('json', data_dir='/home/paperspace/trainingModel/modelDataset/ExtraDataProcessed', split='train')\n","dataset2 = load_dataset('json', data_dir='/home/paperspace/trainingModel/modelDataset/MastodonProcessed', split='train')\n","\n","data = datasets.concatenate_datasets([dataset1, dataset2])\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"content\"], text_target=examples[\"content\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","data = data.map(preprocess_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"_0MOtwf3zdZp"},"source":["Run the cell below to run the training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jq0nX33BmfaC","outputId":"a2822d18-d76a-4591-8323-a6499c7775de"},"outputs":[],"source":["import transformers\n","\n","# needed for gpt-neo-x tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data,\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=8,\n","        warmup_steps=100,\n","        max_steps=500,\n","        learning_rate=1e-4,\n","        fp16=True,\n","        logging_steps=50,\n","        output_dir=\"path/to/save/model\",\n","        optim=\"paged_adamw_8bit\",\n","    ),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","model.gradient_checkpointing_enable()  # Removed the use_reentrant argument\n","model.config.use_cache = False\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p66mZk1RAlOR","outputId":"690765f6-67f5-44d0-f518-c273e3db5976"},"outputs":[],"source":["model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(\"path/to/save/model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2Hllu-bCuN6"},"outputs":[],"source":["lora_config = LoraConfig.from_pretrained('path/to/save/model')\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"markdown","metadata":{"id":"VcGvGqu_QgEz"},"source":["Model Question Input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1TiIH6vAlr_","outputId":"421b2652-b5cf-41ca-d297-8f076e6e263c"},"outputs":[],"source":["text = \"Your question here\"\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","outputs = model.generate(**inputs, max_new_tokens=100)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZHSQX1zQgE0"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
